# -*- coding: utf-8 -*-
"""spam_detector_using_transformer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17yANEbMvh5QwOVemmJGKmsCB_DYkABHx
"""

!pip install --upgrade transformers datasets scikit-learn pandas

import pandas as pd
from sklearn.model_selection import train_test_split
from datasets import Dataset
from sklearn.metrics import accuracy_score, classification_report
from transformers import (
    DistilBertTokenizerFast,
    DistilBertForSequenceClassification,
    Trainer,
    TrainingArguments
)

df=pd.read_csv('spam_ham_dataset.csv')
df.head()

df = df.drop(columns=['Unnamed: 0'])

df = df.groupby("label").apply(lambda x: x.sample(n=300, random_state=42)).reset_index(drop=True)

df.isnull().sum()

df = df.rename(columns={'label': 'category', 'text': 'message', 'label_num': 'label'})

df['category'].value_counts()

df['category'].value_counts(normalize=True) * 100

df = df.drop_duplicates()

# Split into train, validation, and test sets
train_texts, temp_texts, train_labels, temp_labels = train_test_split(
    df['message'], df['label'], test_size=0.3, random_state=42, shuffle=True
)
val_texts, test_texts, val_labels, test_labels = train_test_split(
    temp_texts, temp_labels, test_size=0.5, random_state=42
)

# Convert to datasets
train_df = pd.DataFrame({'text': train_texts, 'label': train_labels})
val_df   = pd.DataFrame({'text': val_texts, 'label': val_labels})
test_df  = pd.DataFrame({'text': test_texts, 'label': test_labels})

train_dataset = Dataset.from_pandas(train_df)
val_dataset = Dataset.from_pandas(val_df)
test_dataset = Dataset.from_pandas(test_df)

from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)

# Tokenize your dataset
def tokenize(batch):
    return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=128)

# Example: Convert pandas dataframe to Dataset (if not already)
# from datasets import Dataset
# train_dataset = Dataset.from_pandas(train_df)

train_dataset = train_dataset.map(tokenize, batched=True)
train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])

# Define metrics
def compute_metrics(pred):
    labels = pred.label_ids
    preds = np.argmax(pred.predictions, axis=1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=4,
    per_device_train_batch_size=16,
    logging_dir='./logs',
    logging_steps=10
)


# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    compute_metrics=compute_metrics
)

# Start training
trainer.train()

trainer.save_model("./spam_classifier_model")
tokenizer.save_pretrained("./spam_classifier_model")

import torch
def predict_message():
    msg = input("Enter a message: ")
    enc = tokenizer(msg, truncation=True, padding='max_length', max_length=128, return_tensors="pt")

    # Check if a CUDA-enabled GPU is available and move tensors to the GPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    enc = {k: v.to(device) for k, v in enc.items()}
    model.to(device) # Ensure the model is also on the correct device

    with torch.no_grad():
        output = model(**enc)
        pred = output.logits.argmax(-1).item()
    print("Prediction:", "Spam" if pred == 1 else "Ham")
    # Loop to test multiple messages
while True:
    predict_message()
    again = input("Do you want to test another message? (y/n): ").strip().lower()
    if again != 'y':
        break

# Format test dataset if not already
test_dataset = test_dataset.map(tokenize, batched=True)
test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])

# Evaluate using trainer
eval_results = trainer.evaluate(eval_dataset=test_dataset)

print("ðŸ“Š Test Accuracy:", eval_results['eval_accuracy'])

